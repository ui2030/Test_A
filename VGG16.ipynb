{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ui2030/Test_A/blob/main/VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "VPMtwv3aGkdc",
        "outputId": "c2fe9279-7eaf-4fd1-b0e5-5ca53f5d30e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n",
            "(0,)\n",
            "(0, 1)\n",
            "(0,)\n",
            "[]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-422dca99feb7>\u001b[0m in \u001b[0;36m<cell line: 161>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;31m# print(training_labels.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m X_train, X_valid, y_train, y_valid = train_test_split(training_images,\n\u001b[0m\u001b[1;32m    162\u001b[0m                                                       \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                                                       \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ],
      "source": [
        "from glob import glob\n",
        "\n",
        "import PIL\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import seaborn as sns\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.layers import Dense, Flatten, MaxPooling2D\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "\n",
        "\n",
        "\n",
        "def save_history(history, model_name):\n",
        "    hist_df = pd.DataFrame(history.history)\n",
        "\n",
        "    # save to json:\n",
        "    hist_json_file = model_name + '_history.json'\n",
        "    with open(hist_json_file, mode='w') as f:\n",
        "        hist_df.to_json(f)\n",
        "\n",
        "    # or save to csv:\n",
        "    hist_csv_file = model_name + '_history.csv'\n",
        "    with open(hist_csv_file, mode='w') as f:\n",
        "        hist_df.to_csv(f)\n",
        "\n",
        "\n",
        "def plot_accuracy_from_history(history, history_file_name):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    sns.lineplot(x=epochs, y=acc, label='Training Accuracy')\n",
        "    sns.lineplot(x=epochs, y=val_acc, label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.savefig(history_file_name + '_acc.png')\n",
        "    plt.figure()\n",
        "\n",
        "\n",
        "def plot_loss_from_history(history, history_file_name):\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs = range(len(loss))\n",
        "\n",
        "    sns.lineplot(x=epochs, y=loss, label='Training Loss')\n",
        "    sns.lineplot(x=epochs, y=val_loss, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(history_file_name + '_loss.png')\n",
        "    plt.figure()\n",
        "\n",
        "\n",
        "def do_history_stuff(history, history_file_name):\n",
        "    save_history(history, history_file_name)\n",
        "    plot_accuracy_from_history(history, history_file_name)\n",
        "    plot_loss_from_history(history, history_file_name)\n",
        "\n",
        "\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/final_symbols_split_ttv/'\n",
        "\n",
        "training_images = []\n",
        "training_labels = []\n",
        "\n",
        "for filename in glob(path + \"*\"):\n",
        "    for img in glob(filename + \"/*.png\"):\n",
        "        an_img = PIL.Image.open(img).convert('RGB') # read img\n",
        "        an_img = an_img.resize((32, 32))\n",
        "        img_array = np.array(an_img)  # img to array\n",
        "        training_images.append(img_array)  # append array to training_images\n",
        "        label = filename.split('\\\\')[-1]  # get label\n",
        "        training_labels.append(label)  # append label\n",
        "\n",
        "\n",
        "training_images = np.array(training_images)\n",
        "training_labels = np.array(training_labels)\n",
        "\n",
        "le = LabelEncoder()\n",
        "training_labels = le.fit_transform(training_labels)\n",
        "training_labels = training_labels.reshape(-1, 1)\n",
        "\n",
        "print(training_images.shape)\n",
        "print(training_labels.shape)\n",
        "\n",
        "path = '/content/drive/MyDrive/MLDL/cacpj/dataset/test/'\n",
        "\n",
        "test_images = []\n",
        "test_idx = []\n",
        "\n",
        "flist = sorted(glob(path + '*.png'))\n",
        "\n",
        "for filename in flist:\n",
        "    an_img = PIL.Image.open(filename).convert('RGB')  # read333333333333333 img\n",
        "    an_img = an_img.resize((32, 32))\n",
        "    img_array = np.array(an_img)  # img to array\n",
        "    test_images.append(img_array)  # append array to training_images\n",
        "    label = filename.split('\\\\')[-1]  # get id\n",
        "    test_idx.append(label)  # append id\n",
        "\n",
        "test_images = np.array(test_images)\n",
        "\n",
        "print(test_images.shape)\n",
        "print(test_idx[0:5])\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# image_generator = ImageDataGenerator(\n",
        "#     shear_range=0.2,\n",
        "#     zoom_range=0.1,\n",
        "#     width_shift_range=0.05,\n",
        "#     height_shift_range=0.05,\n",
        "#     cval=255,\n",
        "#     fill_mode='constant'\n",
        "# )\n",
        "\n",
        "# training_image_aug = image_generator.flow(training_images, np.zeros(42749), batch_size=42749, shuffle=False, seed = 42).next()[0]\n",
        "\n",
        "# training_images = np.concatenate((training_images,\n",
        "#                                   training_image_aug))\n",
        "\n",
        "# training_labels = np.concatenate((training_labels,\n",
        "#                                   training_labels))\n",
        "\n",
        "\n",
        "\n",
        "training_labels = tf.one_hot(training_labels, 14) #one-hot 기법 적용\n",
        "training_labels = np.array(training_labels)\n",
        "training_labels = training_labels.reshape(-1,14) #one-hot 기법을 적용했다면, shape을 바꿔줍니다.\n",
        "\n",
        "# print(training_images.shape)\n",
        "# print(training_labels.shape)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(training_images,\n",
        "                                                      training_labels,\n",
        "                                                      test_size=0.1,\n",
        "                                                      stratify = training_labels,\n",
        "                                                      random_state=42,\n",
        "                                                      shuffle = True)\n",
        "\n",
        "X_test = test_images\n",
        "\n",
        "print('X_train 크기:',X_train.shape)\n",
        "print('y_train 크기:',y_train.shape)\n",
        "print('X_valid 크기:',X_valid.shape)\n",
        "print('y_valid 크기:',y_valid.shape)\n",
        "print('X_test  크기:',X_test.shape)\n",
        "\n",
        "X_train = X_train / 255.0\n",
        "X_valid = X_valid / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "\n",
        "\n",
        "base_model = VGG16(include_top=False, pooling = 'avg' , input_shape = (32, 32, 3), weights = \"imagenet\")\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "vgg_x = Flatten()(base_model.output)\n",
        "vgg_x = Dense(64, activation= 'relu')(vgg_x)\n",
        "vgg_x = Dense(14, activation = 'softmax')(vgg_x)\n",
        "model_vgg = tf.keras.Model(base_model.input, vgg_x)\n",
        "\n",
        "model_vgg.summary()\n",
        "\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "model_vgg.compile(optimizer = tf.keras.optimizers.Adam(learning_rate= 0.01),\n",
        "                  loss = 'categorical_crossentropy',\n",
        "                  metrics=['acc'])\n",
        "\n",
        "vgg16_history = model_vgg.fit(X_train, y_train, epochs = 1, validation_data=(X_valid, y_valid)\n",
        "                                 ,batch_size= 8)\n",
        "\n",
        "for layer in base_model.layers[15:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "\n",
        "model_vgg.compile(optimizer = tf.keras.optimizers.Adam(learning_rate= 0.001),\n",
        "                  loss = 'categorical_crossentropy',\n",
        "                  metrics=['acc'])\n",
        "\n",
        "vgg16_history = model_vgg.fit(X_train, y_train, epochs = 100, validation_data=(X_valid, y_valid)\n",
        "                                 ,batch_size= 16)\n",
        "\n",
        "\n",
        "do_history_stuff(vgg16_history, 'vgg16_model')\n"
      ]
    }
  ]
}